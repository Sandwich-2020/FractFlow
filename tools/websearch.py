import requests
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse, parse_qs
from mcp.server.fastmcp import FastMCP
from bs4 import BeautifulSoup

# åˆå§‹åŒ–MCPæœåŠ¡å™¨
mcp = FastMCP("web_search_executor")

# ------------------------------------------------------------------------
# ç½‘ç»œå·¥å…·å‡½æ•°
# ------------------------------------------------------------------------

def _get_user_agent() -> str:
    """è¿”å›é€šç”¨çš„User-Agentå¤´ä¿¡æ¯"""
    return "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

def _make_request(url: str, timeout: int = 15) -> requests.Response:
    """
    å‘é€HTTPè¯·æ±‚å¹¶è¿”å›å“åº”
    
    Args:
        url: è¯·æ±‚çš„URL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        requests.Responseå¯¹è±¡
    
    Raises:
        requests.exceptions.RequestException: å½“è¯·æ±‚å¤±è´¥æ—¶
    """
    headers = {"User-Agent": _get_user_agent()}
    response = requests.get(url, headers=headers, timeout=timeout)
    response.raise_for_status()
    return response

@mcp.tool()
async def web_browse(url: str, extract_type: str = "full_text") -> str:
    """
    è·å–å¹¶æå–ç½‘é¡µå†…å®¹
    
    Args:
        url (str): è¦æµè§ˆçš„ç½‘é¡µURL
        extract_type (str, optional): è¦æå–çš„å†…å®¹ç±»å‹ã€‚å¯é€‰å€¼:
                                      "full_text" - æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                                      "title" - ä»…æå–é¡µé¢æ ‡é¢˜
                                      "links" - æå–é¡µé¢ä¸Šæ‰€æœ‰é“¾æ¥
                                      "html" - è¿”å›åŸå§‹HTML
                                      é»˜è®¤ä¸º "full_text"
        
    Returns:
        str: æ ¹æ®extract_typeæå–çš„ç½‘é¡µå†…å®¹ï¼Œæˆ–åœ¨URLæ— æ•ˆæˆ–è¯·æ±‚å¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
        
    Example:
        web_browse("https://www.example.com", extract_type="full_text")
    """
    try:
        # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
        parsed_url = urlparse(url)
        if not parsed_url.scheme or not parsed_url.netloc:
            return "æ— æ•ˆURLã€‚è¯·æä¾›å®Œæ•´URLï¼ŒåŒ…æ‹¬http://æˆ–https://"
        
        # å‘é€è¯·æ±‚
        response = _make_request(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ ¹æ®extract_typeæå–å†…å®¹
        if extract_type == "full_text":
            # ç§»é™¤scriptå’Œstyleå…ƒç´ 
            for script in soup(["script", "style"]):
                script.extract()
            text = soup.get_text(separator="\n", strip=True)
            return text
            
        elif extract_type == "title":
            title = soup.title.string if soup.title else "æœªæ‰¾åˆ°æ ‡é¢˜"
            return title
            
        elif extract_type == "links":
            links = []
            for link in soup.find_all('a'):
                href = link.get('href')
                if href:
                    links.append(f"{link.text.strip()} - {href}")
            return "\n".join(links) if links else "æœªæ‰¾åˆ°é“¾æ¥"
            
        elif extract_type == "html":
            return response.text
            
        else:
            return f"æ— æ•ˆçš„extract_type: {extract_type}ã€‚æœ‰æ•ˆé€‰é¡¹: full_text, title, links, html"
            
    except requests.exceptions.RequestException as e:
        return f"è·å–URLå¤±è´¥: {str(e)}"
    except Exception as e:
        return f"å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
async def web_search(query: str, search_engine: str = "duckduckgo", num_results: int = 5) -> str:
    """
    æ‰§è¡Œç½‘ç»œæœç´¢å¹¶è¿”å›ç›¸å…³ç»“æœ
    
    Args:
        query (str): è¦æœç´¢çš„å…³é”®è¯æˆ–çŸ­è¯­
        search_engine (str, optional): è¦ä½¿ç”¨çš„æœç´¢å¼•æ“
                                      å¯é€‰å€¼: "duckduckgo", "bing", "google"
                                      é»˜è®¤ä¸º "duckduckgo"
        num_results (int, optional): è¦è¿”å›çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º5ä¸ª
        
    Returns:
        str: åŒ…å«æ ‡é¢˜ã€é“¾æ¥å’Œæè¿°çš„æœç´¢ç»“æœï¼Œæˆ–åœ¨æœç´¢å¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
        
    Example:
        web_search("Python ç¼–ç¨‹æ•™ç¨‹", search_engine="duckduckgo", num_results=3)
    """
    try:
        # éªŒè¯num_resultså‚æ•°
        if num_results <= 0:
            return "ç»“æœæ•°é‡å¿…é¡»å¤§äº0"
        
        if num_results > 20:
            num_results = 20
            
        search_engine = search_engine.lower()
        
        if search_engine == "duckduckgo":
            return await _search_duckduckgo(query, num_results)
        elif search_engine == "bing":
            return await _search_bing(query, num_results)
        elif search_engine == "google":
            return await _search_google(query, num_results)
        else:
            return f"ä¸æ”¯æŒçš„æœç´¢å¼•æ“: {search_engine}ã€‚æ”¯æŒçš„é€‰é¡¹: duckduckgo, bing, google"
            
    except requests.exceptions.RequestException as e:
        return f"æœç´¢è¯·æ±‚å¤±è´¥: {str(e)}"
    except Exception as e:
        return f"æ‰§è¡Œæœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

async def _search_duckduckgo(query: str, num_results: int) -> str:
    """
    ä½¿ç”¨DuckDuckGoæœç´¢å¼•æ“æ‰§è¡Œæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: è¦è¿”å›çš„ç»“æœæ•°é‡
        
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
    """
    url = f"https://html.duckduckgo.com/html/?q={query}"
    response = _make_request(url, timeout=15)
    
    soup = BeautifulSoup(response.text, 'html.parser')
    results = []
    
    # è§£æDuckDuckGoçš„æœç´¢ç»“æœ
    for result in soup.select('.result')[:num_results]:
        title_elem = result.select_one('.result__title')
        if not title_elem:
            continue
            
        title = title_elem.get_text(strip=True)
        link_elem = title_elem.select_one('a')
        link = link_elem.get('href') if link_elem else "é“¾æ¥æœªæ‰¾åˆ°"
        
        # å¤„ç†DuckDuckGoçš„é‡å®šå‘é“¾æ¥
        if link.startswith('/'):
            # è¿™æ˜¯DuckDuckGoçš„é‡å®šå‘é“¾æ¥ï¼Œå°è¯•æå–çœŸå®URL
            try:
                parsed = parse_qs(link.split('?', 1)[1])
                if 'uddg' in parsed:
                    link = parsed['uddg'][0]
            except:
                pass
        
        snippet_elem = result.select_one('.result__snippet')
        snippet = snippet_elem.get_text(strip=True) if snippet_elem else "æ²¡æœ‰å¯ç”¨çš„æè¿°"
        
        results.append(f"ğŸ“Œ {title}\nğŸ”— {link}\nğŸ“„ {snippet}\n")
    
    if not results:
        return f"åœ¨DuckDuckGoä¸Šæ²¡æœ‰æ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„ç»“æœ"
        
    return f"ğŸ” '{query}'çš„æœç´¢ç»“æœ:\n\n" + "\n".join(results)

async def _search_bing(query: str, num_results: int) -> str:
    """
    ä½¿ç”¨Bingæœç´¢å¼•æ“æ‰§è¡Œæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: è¦è¿”å›çš„ç»“æœæ•°é‡
        
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
    """
    url = f"https://www.bing.com/search?q={query}"
    response = _make_request(url, timeout=15)
    
    soup = BeautifulSoup(response.text, 'html.parser')
    results = []
    
    # è§£æBingçš„æœç´¢ç»“æœ
    for result in soup.select('.b_algo')[:num_results]:
        title_elem = result.select_one('h2')
        if not title_elem:
            continue
            
        title = title_elem.get_text(strip=True)
        link_elem = title_elem.select_one('a')
        link = link_elem.get('href') if link_elem else "é“¾æ¥æœªæ‰¾åˆ°"
        
        snippet_elem = result.select_one('.b_caption p')
        snippet = snippet_elem.get_text(strip=True) if snippet_elem else "æ²¡æœ‰å¯ç”¨çš„æè¿°"
        
        results.append(f"ğŸ“Œ {title}\nğŸ”— {link}\nğŸ“„ {snippet}\n")
    
    if not results:
        return f"åœ¨Bingä¸Šæ²¡æœ‰æ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„ç»“æœ"
        
    return f"ğŸ” '{query}'çš„æœç´¢ç»“æœ:\n\n" + "\n".join(results)

async def _search_google(query: str, num_results: int) -> str:
    """
    ä½¿ç”¨Googleæœç´¢å¼•æ“æ‰§è¡Œæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        num_results: è¦è¿”å›çš„ç»“æœæ•°é‡
        
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
    """
    url = f"https://www.google.com/search?q={query}&num={min(num_results, 10)}"
    response = _make_request(url, timeout=15)
    
    soup = BeautifulSoup(response.text, 'html.parser')
    results = []
    
    # è§£æGoogleçš„æœç´¢ç»“æœ
    # Googleç»“æ„å¯èƒ½ç»å¸¸å˜åŒ–ï¼Œè¿™é‡Œä½¿ç”¨ä¸€ç§å¸¸è§çš„é€‰æ‹©å™¨
    for result in soup.select('div.g')[:num_results]:
        title_elem = result.select_one('h3')
        if not title_elem:
            continue
            
        title = title_elem.get_text(strip=True)
        
        link_elem = result.select_one('a')
        link = link_elem.get('href') if link_elem else "é“¾æ¥æœªæ‰¾åˆ°"
        if link.startswith('/url?'):
            try:
                parsed = parse_qs(link.split('?', 1)[1])
                if 'q' in parsed:
                    link = parsed['q'][0]
            except:
                pass
        
        snippet_elem = result.select_one('div.VwiC3b')
        snippet = snippet_elem.get_text(strip=True) if snippet_elem else "æ²¡æœ‰å¯ç”¨çš„æè¿°"
        
        results.append(f"ğŸ“Œ {title}\nğŸ”— {link}\nğŸ“„ {snippet}\n")
    
    if not results:
        return f"åœ¨Googleä¸Šæ²¡æœ‰æ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„ç»“æœ"
        
    return f"ğŸ” '{query}'çš„æœç´¢ç»“æœ:\n\n" + "\n".join(results)

# ä¸»å…¥å£
if __name__ == "__main__":
    mcp.run(transport="stdio")
